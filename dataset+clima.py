# -*- coding: utf-8 -*-
"""Dataset_FInal+Clima

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aUfermYqW_V8v3k6VE0bmEY6fWp9mwPf
"""

!pip install kagglehub pyarrow requests

# INSTALACIÓN DE LIBRERÍAS (solo en Colab / notebooks)
# kagglehub: descarga datasets de Kaggle
# pyarrow: lectura eficiente de archivos Parquet
# requests: llamadas a APIs externas (clima, geocoding)

# os: manejo de rutas y archivos
# pandas: manipulación de dataframes
# pyarrow.parquet: lectura de parquet por bloques
# requests: consumo de APIs


import kagglehub
import os
import pyarrow.parquet as pq
import pandas as pd
import requests

# Descargar dataset
# Descarga el dataset de retrasos de vuelos
# Devuelve la ruta local donde queda almacenado

path = kagglehub.dataset_download("arvindnagaonkar/flight-delay")

print("Dataset descargado en:")
print(path)

# Lista los archivos disponibles dentro del dataset descargado

os.listdir(path)

# Construimos la ruta al archivo parquet
# ParquetFile permite leer el dataset sin cargarlo entero en memoria

PARQUET_FILE = os.path.join(path, "Flight_Delay.parquet")

pf = pq.ParquetFile(PARQUET_FILE)


# Número máximo de filas que queremos cargar

N_SAMPLE = 100_000

# Nos quedamos solo con columnas relevantes para ML y clima

COLUMNS = [
    "FlightDate",
    "OriginCityName",
    "CRSDepTime",
    "DepDelay",
    "Distance",
    "Marketing_Airline_Network"
]

# rows: guardará los bloques de datos
# rows_read: contador de filas leídas

rows = []
rows_read = 0

# Leemos el parquet en bloques de 50k filas
# Esto evita problemas de memoria en Colab

for batch in pf.iter_batches(batch_size=50_000, columns=COLUMNS):
    df_batch = batch.to_pandas()        # Convertimos el bloque a DataFrame


# Calculamos cuántas filas nos faltan
    remaining = N_SAMPLE - rows_read
    if remaining <= 0:       # Si ya alcanzamos el límite, salimos del loop
        break

# Si el bloque es más grande de lo necesario, lo muestreamos

    if len(df_batch) > remaining:
        df_batch = df_batch.sample(remaining, random_state=42)

    # Guardamos el bloque

    rows.append(df_batch)
    rows_read += len(df_batch)

# Concatenamos los bloques en un solo DataFrame
# Convertimos nombres de columnas a minúsculas

df = pd.concat(rows, ignore_index=True)
df.columns = df.columns.str.lower()

df.shape

df_base = df.copy()

# Fecha
df_base["flightdate"] = pd.to_datetime(df_base["flightdate"])

# Hora programada (sin leakage)
df_base["hour"] = df_base["crsdeptime"] // 100

# Target
df_base["delayed"] = (df_base["depdelay"] >= 15).astype(int)

# Subsample MVP
df_base = df_base.sample(5_000, random_state=42)

df_base.head()

# "Dallas, TX" -> "Dallas"
# Necesario para el geocoding

df_base["city_clean"] = (
    df_base["origincityname"]
    .astype(str)
    .str.split(",")
    .str[0]
    .str.strip()
)

df_base[["origincityname", "city_clean"]].head()

# Devuelve latitud y longitud a partir del nombre de la ciudad

def geocode_city(city):
    url = "https://geocoding-api.open-meteo.com/v1/search"
    params = {
        "name": city,
        "count": 1,
        "language": "en",
        "format": "json",
        "country": "US"
    }

    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    if "results" not in data or len(data["results"]) == 0:
        return None, None

    return data["results"][0]["latitude"], data["results"][0]["longitude"]

# Usamos solo ciudades únicas para reducir llamadas a la API

cities = df_base["city_clean"].unique()

geo_rows = []

for city in cities:
    lat, lon = geocode_city(city)
    if lat is not None:
        geo_rows.append({
            "city_clean": city,
            "latitude": lat,
            "longitude": lon
        })

df_geo = pd.DataFrame(geo_rows)
df_geo.head()

#merge coordenadas y dataset

df_base = df_base.merge(df_geo, on="city_clean", how="left")
df_base.head()

# Obtiene temperatura, lluvia y viento por coordenadas

OPENWEATHER_API_KEY = "3aa4a7e2a01232dd9e5bcdb0c3877afe"

def get_weather_openweather(lat, lon, api_key):
    url = "https://api.openweathermap.org/data/2.5/weather"
    params = {
        "lat": lat,
        "lon": lon,
        "appid": api_key,
        "units": "metric"
    }

    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()

    return {
        "temp_mean": data["main"]["temp"],
        "precipitation": data.get("rain", {}).get("1h", 0.0),
        "wind_speed": data["wind"]["speed"]
    }

# Obtenemos clima por ciudad

weather_rows = []

cities_weather = (
    df_base[["city_clean", "latitude", "longitude"]]
    .dropna()
    .drop_duplicates()
)

for _, row in cities_weather.iterrows():
    try:
        weather = get_weather_openweather(
            row.latitude,
            row.longitude,
            OPENWEATHER_API_KEY
        )

        weather_rows.append({
            "city_clean": row.city_clean,
            "temp_mean": weather["temp_mean"],
            "precipitation": weather["precipitation"],
            "wind_speed": weather["wind_speed"]
        })

    except Exception as e:
        print(f"Error clima {row.city_clean}: {e}")

df_weather = pd.DataFrame(weather_rows)
df_weather.head()

# Merge vuelos + clima

df_final = df_base.merge(
    df_weather,
    on="city_clean",
    how="left"
)

df_final.head()

# Eliminamos filas sin información climática

df_ml = df_final.dropna(
    subset=["temp_mean", "precipitation", "wind_speed"]
)

df_ml.shape

OUTPUT_CSV = "dataset_vuelos_clima_final.csv"
df_ml.to_csv(OUTPUT_CSV, index=False)

OUTPUT_CSV

from google.colab import files
files.download(OUTPUT_CSV)