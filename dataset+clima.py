# -*- coding: utf-8 -*-
"""rafadata_clima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aQQox5k8scRqlVUaqpwNF2543xsT6YTA
"""

#Descargo el dataset

#pyarrow: lee parquet

#requests: para llamar a API


!pip install kagglehub pyarrow requests

import kagglehub


path = kagglehub.dataset_download("arvindnagaonkar/flight-delay")
print(path)

import pandas as pd
import pyarrow.parquet as pq

parquet_file = "/kaggle/input/flight-delay/Flight_Delay.parquet"
n_sample = 100_000   #Limito filas

pf = pq.ParquetFile(parquet_file)  #Abre parte del parquet si cargarse la memoria

#columnas que necesitamos

columns_needed = [
    "FlightDate",
    "OriginCityName",
    "CRSDepTime",
    "DepDelay",
    "Distance",
    "Marketing_Airline_Network"
]

#loop muestreo. Lee parquet por bloques, junta filas, no carga todo el dataset.

rows = []
rows_read = 0

for batch in pf.iter_batches(batch_size=50_000, columns=columns_needed):
    df_batch = batch.to_pandas()
    remaining = n_sample - rows_read
    if remaining <= 0:
        break
    if len(df_batch) > remaining:
        df_batch = df_batch.sample(remaining, random_state=42)
    rows.append(df_batch)
    rows_read += len(df_batch)

#dataframe resultado uniendo batches

df = pd.concat(rows, ignore_index=True)
df.shape

#Normalizo nombres a minusculas

df.columns = df.columns.str.lower()

#feature engineering: convertir fecha, sacar hora, crear delayed

df_base = df.copy()
df_base["flightdate"] = pd.to_datetime(df_base["flightdate"])
df_base["hour"] = df_base["crsdeptime"] // 100
df_base["delayed"] = (df_base["depdelay"] >= 15).astype(int)


#muestra

df_base = df_base.sample(5_000, random_state=42)
df_base.head()

#geocoding

import requests

def geocode_city(city):
    url = "https://geocoding-api.open-meteo.com/v1/search"
    params = {
        "name": city,
        "count": 1,
        "language": "en",
        "format": "json",
        "country": "US"
    }

    r = requests.get(url, params=params)
    r.raise_for_status()
    data = r.json()

    if "results" not in data or len(data["results"]) == 0:
        return None, None

    return data["results"][0]["latitude"], data["results"][0]["longitude"]

#Limpio ciudades

df_base["city_clean"] = (
    df_base["origincityname"]
    .astype(str)
    .str.split(",")
    .str[0]
    .str.strip()
)
cities = df_base["city_clean"].unique()[:20]

#selecciono ciudades

cities = df_base["city_clean"].unique()[:20]

#geocoding loop: latitud y longiyud

geo_rows = []
for city in cities:
    lat, lon = geocode_city(city)
    if lat is not None:
        geo_rows.append({
            "city_clean": city,
            "latitude": lat,
            "longitude": lon
        })

df_geo = pd.DataFrame(geo_rows)
df_geo

#Merge con coordenadas

df_base = df_base.merge(df_geo, on="city_clean", how="left")
df_base[["city_clean", "latitude", "longitude"]].head()

#Claves Ãºnicas del clima para no duplicar

weather_keys = (
    df_base[["city_clean", "latitude", "longitude", "flightdate"]]
    .dropna()
    .drop_duplicates()
)

#agrupamos fechas minimas y max por ciudad

city_ranges = (
    weather_keys
    .groupby(["city_clean", "latitude", "longitude"])
    .agg(
        start_date=("flightdate", "min"),
        end_date=("flightdate", "max")
    )
    .reset_index()
)

#llamamos api una vez por ciudad y obtenemos clima diario

def get_weather_range(lat, lon, start_date, end_date):
    url = "https://archive-api.open-meteo.com/v1/archive"
    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_date,
        "end_date": end_date,
        "daily": [
            "temperature_2m_mean",
            "precipitation_sum",
            "windspeed_10m_mean"
        ],
        "timezone": "UTC"
    }

    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    d = r.json()["daily"]

    return pd.DataFrame({
        "flightdate": pd.to_datetime(d["time"]),
        "temp_mean": d["temperature_2m_mean"],
        "precipitation": d["precipitation_sum"],
        "wind_speed": d["windspeed_10m_mean"]
    })

#descargamos clima por ciudad

weather_frames = []

for _, row in city_ranges.iterrows():
    try:
        df_city_weather = get_weather_range(
            row.latitude,
            row.longitude,
            row.start_date.strftime("%Y-%m-%d"),
            row.end_date.strftime("%Y-%m-%d")
        )
        df_city_weather["city_clean"] = row.city_clean
        weather_frames.append(df_city_weather)
    except Exception as e:
        print(row.city_clean, e)

df_weather = pd.concat(weather_frames, ignore_index=True)
df_weather.shape

#merge vuelos y clima

df_final = df_base.merge(
    df_weather,
    on=["city_clean", "flightdate"],
    how="left"
)

df_final.isna().mean()

#eliminamos filas sin clima correcto

df_ml = df_final.dropna(
    subset=["temp_mean", "precipitation", "wind_speed"]
)

#guardo parquet


df_ml.to_parquet("flights_with_weather.parquet", index=False)
df_ml.shape

#pasamos a csv y descargamos

import pandas as pd

df = pd.read_parquet("flights_with_weather.parquet")
df.to_csv("flights_with_weather.csv", index=False)

from google.colab import files
files.download("flights_with_weather.csv")