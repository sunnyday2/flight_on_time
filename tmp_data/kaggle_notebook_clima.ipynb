{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 6310492,
          "sourceType": "datasetVersion",
          "datasetId": 3620019,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 14458291,
          "sourceType": "datasetVersion",
          "datasetId": 9229756
        },
        {
          "sourceId": 14458305,
          "sourceType": "datasetVersion",
          "datasetId": 9222373
        }
      ],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunnyday2/flight_on_time/blob/desarrollo/flightontime_hackaton_e32_alura_latam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kaggle Notebook para un pre-procesamiento pesado**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EDkKLVxdjl2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado a que Colab no tiene suficientes recursos para elaborar el dataset a base de millones registros, usa Kaggle para:\n",
        "* Lectura de datasets grandes (Parquet, CSV > varios GB)\n",
        "* Limpieza inicial\n",
        "* Feature engineering\n",
        "* Agregaciones\n",
        "* Generaci√≥n de datasets intermedios\n",
        "\n",
        "**Ventajas**\n",
        "* Datasets montados nativamente (sin bugs de disco)\n",
        "* M√°s I/O estable\n",
        "* Entorno reproducible\n",
        "* Ideal para EDA y pipelines de datos"
      ],
      "metadata": {
        "id": "r3K42tV8jl2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. üìö Librer√≠as y configuraci√≥n**\n",
        "---"
      ],
      "metadata": {
        "id": "75o1DFOUYYQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar las librer√≠as necesarias para leer datos y construir el dataset"
      ],
      "metadata": {
        "id": "pyAJuwtqlZvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub pyarrow requests"
      ],
      "metadata": {
        "id": "tCtZZ84VWiq0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:08:40.505470Z",
          "iopub.execute_input": "2026-01-10T14:08:40.505793Z",
          "iopub.status.idle": "2026-01-10T14:08:45.530452Z",
          "shell.execute_reply.started": "2026-01-10T14:08:40.505764Z",
          "shell.execute_reply": "2026-01-10T14:08:45.529261Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consolidar todas las librer√≠as en una sola secci√≥n para evitar imports repetidos"
      ],
      "metadata": {
        "id": "wTz30Mf-lpc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import math\n",
        "import fcntl  # En Windows reemplaza por msvcrt.locking si lo necesitas"
      ],
      "metadata": {
        "id": "vaAM2qirnwj6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:08:48.223197Z",
          "iopub.execute_input": "2026-01-10T14:08:48.224183Z",
          "iopub.status.idle": "2026-01-10T14:08:50.663494Z",
          "shell.execute_reply.started": "2026-01-10T14:08:48.224123Z",
          "shell.execute_reply": "2026-01-10T14:08:50.662582Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. ‚¨áÔ∏è Descargar el dataset presentado por Rafael para el analisis y confeci√≥n de uno mas completo**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1AJ6n-MRWfkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extraer porciones de data por a√±os y cargar por bloques"
      ],
      "metadata": {
        "id": "4iKvg7yyxKHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"arvindnagaonkar/flight-delay\")\n",
        "print(\"Dataset descargado en:\")\n",
        "print(path)\n",
        "\n",
        "# Lista los archivos disponibles dentro del dataset descargado\n",
        "os.listdir(path)\n",
        "\n",
        "# Construimos la ruta al archivo parquet\n",
        "# ParquetFile permite leer el dataset sin cargarlo entero en memoria\n",
        "PARQUET_FILE = os.path.join(path, \"Flight_Delay.parquet\")\n",
        "\n",
        "OUT_FILE = \"df_sample_equal_year.parquet\"\n",
        "\n",
        "BATCH_SIZE = 200_000\n",
        "YEAR_COL = \"Year\"\n",
        "\n",
        "# -------- 1) Primer pase: contar filas por Year (streaming) --------\n",
        "pf = pq.ParquetFile(PARQUET_FILE)\n",
        "counts = {}\n",
        "\n",
        "for batch in pf.iter_batches(batch_size=BATCH_SIZE, columns=[YEAR_COL]):\n",
        "    years = batch.column(0).to_numpy()\n",
        "    uniq, cnt = np.unique(years, return_counts=True)\n",
        "    for y, c in zip(uniq, cnt):\n",
        "        counts[int(y)] = counts.get(int(y), 0) + int(c)\n",
        "\n",
        "min_per_year = min(counts.values())  # mismo tama√±o por Year\n",
        "\n",
        "# (opcional) si quieres una fracci√≥n del m√≠nimo:\n",
        "# SAMPLE_FRAC = 0.10\n",
        "# min_per_year = int(min_per_year * SAMPLE_FRAC)\n",
        "\n",
        "print(\"Cantidad de registros en a√±o:\", counts)\n",
        "print(\"Registros por A√±o a guardar:\", min_per_year)\n",
        "\n",
        "# -------- 2) Segundo pase: construir salida con N filas por Year --------\n",
        "pf = pq.ParquetFile(PARQUET_FILE)\n",
        "\n",
        "kept = {y: 0 for y in counts.keys()}\n",
        "writer = None\n",
        "\n",
        "for batch in pf.iter_batches(batch_size=BATCH_SIZE):  # lee todas las columnas\n",
        "    df_chunk = batch.to_pandas()\n",
        "\n",
        "    # seleccionar filas manteniendo cupo por year\n",
        "    parts = []\n",
        "    for y, g in df_chunk.groupby(YEAR_COL, sort=False):\n",
        "        y = int(y)\n",
        "        remaining = min_per_year - kept.get(y, 0)\n",
        "        if remaining <= 0:\n",
        "            continue\n",
        "        take = g.iloc[:remaining]  # determin√≠stico (primeras filas)\n",
        "        kept[y] = kept.get(y, 0) + len(take)\n",
        "        parts.append(take)\n",
        "\n",
        "    if not parts:\n",
        "        # si no hay nada que guardar de este batch, seguir\n",
        "        continue\n",
        "\n",
        "    out_df = pd.concat(parts, ignore_index=True)\n",
        "\n",
        "    out_table = pa.Table.from_pandas(out_df, preserve_index=False)\n",
        "\n",
        "    if writer is None:\n",
        "        writer = pq.ParquetWriter(OUT_FILE, out_table.schema)\n",
        "    writer.write_table(out_table)\n",
        "\n",
        "    # early stop: si ya completaste todos los a√±os, corta\n",
        "    if all(v >= min_per_year for v in kept.values()):\n",
        "        break\n",
        "\n",
        "if writer is not None:\n",
        "    writer.close()\n",
        "\n",
        "print(\"La generaci√≥n del archivo se ha terminado. Archivo creado: \", OUT_FILE)\n",
        "print(\"Cantidades de las muestras seleccionadas: \", kept)"
      ],
      "metadata": {
        "id": "oIaId7UIWfRP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:08:56.150942Z",
          "iopub.execute_input": "2026-01-10T14:08:56.151441Z",
          "iopub.status.idle": "2026-01-10T14:09:27.505477Z",
          "shell.execute_reply.started": "2026-01-10T14:08:56.151413Z",
          "shell.execute_reply": "2026-01-10T14:09:27.504585Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. üßæ Leer el archivo de muestras**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FlaKjXw7oAP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir las muestras en un `DataFrame`"
      ],
      "metadata": {
        "id": "dEJYOdOOp2qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Espere. Estoy leendo el archivo...')\n",
        "df = pd.read_parquet(OUT_FILE)\n",
        "\n",
        "print(\"La lectura ha terminado.\")"
      ],
      "metadata": {
        "id": "bu9pmZ6DHzJC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:09:35.366966Z",
          "iopub.execute_input": "2026-01-10T14:09:35.367830Z",
          "iopub.status.idle": "2026-01-10T14:09:40.200429Z",
          "shell.execute_reply.started": "2026-01-10T14:09:35.367796Z",
          "shell.execute_reply": "2026-01-10T14:09:40.199505Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cambiar los nombres de las columnas a min√∫sculas"
      ],
      "metadata": {
        "id": "YcouNJkwYoW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.lower()"
      ],
      "metadata": {
        "id": "KZAhjVNrac8s",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:09:43.010354Z",
          "iopub.execute_input": "2026-01-10T14:09:43.010729Z",
          "iopub.status.idle": "2026-01-10T14:09:43.015835Z",
          "shell.execute_reply.started": "2026-01-10T14:09:43.010698Z",
          "shell.execute_reply": "2026-01-10T14:09:43.014955Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. üëÄ Inspecci√≥n visual de los datos**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TsuXHYG0xY45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizar las primeras 5 filas"
      ],
      "metadata": {
        "id": "2XCkGj62znZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kK58QXUrzpez",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:09:47.048844Z",
          "iopub.execute_input": "2026-01-10T14:09:47.049241Z",
          "iopub.status.idle": "2026-01-10T14:09:47.088878Z",
          "shell.execute_reply.started": "2026-01-10T14:09:47.049211Z",
          "shell.execute_reply": "2026-01-10T14:09:47.087902Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øCu√°ntas filas y columnas hay?"
      ],
      "metadata": {
        "id": "gJSGQx-I1APT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cantidad de columnas y filas: {df.shape}\")"
      ],
      "metadata": {
        "id": "ZrIN7YSC1DVp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:09:50.493189Z",
          "iopub.execute_input": "2026-01-10T14:09:50.493520Z",
          "iopub.status.idle": "2026-01-10T14:09:50.498656Z",
          "shell.execute_reply.started": "2026-01-10T14:09:50.493486Z",
          "shell.execute_reply": "2026-01-10T14:09:50.497576Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qu√© tipo de datos contiene cada una: ¬øson n√∫meros, fechas, texto?"
      ],
      "metadata": {
        "id": "FMUupA981Jvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "zrJH4G3C0wsj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:09:54.521921Z",
          "iopub.execute_input": "2026-01-10T14:09:54.522619Z",
          "iopub.status.idle": "2026-01-10T14:09:54.530365Z",
          "shell.execute_reply.started": "2026-01-10T14:09:54.522583Z",
          "shell.execute_reply": "2026-01-10T14:09:54.529559Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Que distribuci√≥n de data hay por cada a√±o"
      ],
      "metadata": {
        "id": "4K4wtJj5VZFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"year\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "9tdvpYzr4G2j",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:09:57.940573Z",
          "iopub.execute_input": "2026-01-10T14:09:57.941300Z",
          "iopub.status.idle": "2026-01-10T14:09:58.030021Z",
          "shell.execute_reply.started": "2026-01-10T14:09:57.941266Z",
          "shell.execute_reply": "2026-01-10T14:09:58.029088Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estad√≠stica b√°sica: medias, medianas y desviaciones est√°ndar para entender la distribuci√≥n de los n√∫meros"
      ],
      "metadata": {
        "id": "I-A1tKgj2SY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "KzpOw1gAzUkk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:10:01.118131Z",
          "iopub.execute_input": "2026-01-10T14:10:01.118442Z",
          "iopub.status.idle": "2026-01-10T14:10:10.433364Z",
          "shell.execute_reply.started": "2026-01-10T14:10:01.118417Z",
          "shell.execute_reply": "2026-01-10T14:10:10.432417Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. üß© Creaci√≥n de nuevas variables (Feature Engineering)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B5c95ZJb9dQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Se crea la variable temporal `hour`, la variable objetivo `dalayed` si el retraso en salida esta igual o may√≥r de 15 minutos"
      ],
      "metadata": {
        "id": "8lgN6N9Zss3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear variable temporal 'hour' a partir de la hora programada\n",
        "df[\"hour\"] = df.select_dtypes(include='number')[\"crsdeptime\"] // 100  # solo la hora\n",
        "\n",
        "# Variable objetivo 'delayed': 1 si el retraso en salida >= 15 min\n",
        "df[\"delayed\"] = (df.select_dtypes(include='number')[\"depdelay\"] >= 15).astype(int)\n",
        "\n",
        "# Probabilidad de delay en la muestra\n",
        "delay_rate = df[\"delayed\"].mean()\n",
        "print(f\"Tasa de retraso: {delay_rate:.4f}\")\n",
        "\n",
        "# Ver valores √∫nicos de la variable 'delayed'\n",
        "unique_values = df[\"delayed\"].unique()\n",
        "print(\"Valores √∫nicos en 'delayed':\", unique_values)"
      ],
      "metadata": {
        "id": "996399IoS2Nu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:10:18.110553Z",
          "iopub.execute_input": "2026-01-10T14:10:18.110867Z",
          "iopub.status.idle": "2026-01-10T14:10:20.635214Z",
          "shell.execute_reply.started": "2026-01-10T14:10:18.110841Z",
          "shell.execute_reply": "2026-01-10T14:10:20.634214Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisamos la probilidad de retraso en diferentes horas"
      ],
      "metadata": {
        "id": "wgux2r9KtX-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por hora y calcular la probabilidad de retraso\n",
        "hour_delay = (\n",
        "    df.groupby(\"hour\")[\"delayed\"]\n",
        "    .mean()\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "# Mostrar la tabla de probabilidades por hora\n",
        "print(hour_delay)\n",
        "\n",
        "hour_std = hour_delay.std()\n",
        "print(f\"\\nLa probabilidad de retraso de los vuelos en promedio\\na lo largo de las diferentes horas del d√≠a: {hour_std:.2%}\")"
      ],
      "metadata": {
        "id": "nb4ekBEemxbX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:10:24.016407Z",
          "iopub.execute_input": "2026-01-10T14:10:24.016725Z",
          "iopub.status.idle": "2026-01-10T14:10:24.294794Z",
          "shell.execute_reply.started": "2026-01-10T14:10:24.016699Z",
          "shell.execute_reply": "2026-01-10T14:10:24.293831Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear bins de distancia"
      ],
      "metadata": {
        "id": "3Kq_1-lEtPow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear bins de distancia (5 quintiles)\n",
        "df[\"distance_bin\"] = pd.qcut(df.select_dtypes(include='number')[\"distance\"], q=5)\n",
        "\n",
        "# Calcular probabilidad de retraso por rango de distancia\n",
        "distance_delay = (\n",
        "    df.groupby(\"distance_bin\", observed=True)[\"delayed\"]\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "print(distance_delay)"
      ],
      "metadata": {
        "id": "9M2vpXvcqM1B",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:10:29.973032Z",
          "iopub.execute_input": "2026-01-10T14:10:29.973376Z",
          "iopub.status.idle": "2026-01-10T14:10:32.415213Z",
          "shell.execute_reply.started": "2026-01-10T14:10:29.973349Z",
          "shell.execute_reply": "2026-01-10T14:10:32.414174Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear columna con dia de la semana del vuelo"
      ],
      "metadata": {
        "id": "A_0btePCtkvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir a datetime (ya hecho en df, pero aseguramos si df_numeric no lo tiene)\n",
        "# Asumimos que df_numeric y df tienen el mismo √≠ndice y n√∫mero de filas\n",
        "# y que df['flightdate'] ya es datetime de un paso anterior\n",
        "df[\"day_of_week\"] = pd.to_datetime(df[\"flightdate\"]).dt.dayofweek\n",
        "\n",
        "print(\"Distribuci√≥n de vuelos puntuales y atrasados:\")\n",
        "df[\"delayed\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "XA93dVZVrLPb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:10:35.215540Z",
          "iopub.execute_input": "2026-01-10T14:10:35.215890Z",
          "iopub.status.idle": "2026-01-10T14:10:36.905571Z",
          "shell.execute_reply.started": "2026-01-10T14:10:35.215862Z",
          "shell.execute_reply": "2026-01-10T14:10:36.904527Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extraer c√≥digo de estado `state_clean` y de la `state_clean` ciudad a partir de `origincityname` para una fusi√≥n con las ubicaciones de los aeropuertos"
      ],
      "metadata": {
        "id": "4oqfh1yPQqn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Dallas, TX\" -> \"Dallas\"\n",
        "# Necesario para el geocoding\n",
        "\n",
        "tmp = (\n",
        "    df[\"origincityname\"]\n",
        "    .astype(str)\n",
        "    .str.split(\",\", n=1, expand=True)\n",
        ")\n",
        "\n",
        "df[\"state_clean\"] = tmp[0].str.strip()\n",
        "df[\"state_clean\"] = tmp[1].str.strip()  # lo que va despu√©s de la coma\n",
        "\n",
        "df[[\"origincityname\", \"city_clean\", \"state_clean\"]].head()"
      ],
      "metadata": {
        "id": "yqU0VQOzUvDS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:10:39.907811Z",
          "iopub.execute_input": "2026-01-10T14:10:39.908177Z",
          "iopub.status.idle": "2026-01-10T14:11:02.303134Z",
          "shell.execute_reply.started": "2026-01-10T14:10:39.908146Z",
          "shell.execute_reply": "2026-01-10T14:11:02.302312Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descargar y revisar datos con ubicaci√≥n de los aeropuertos"
      ],
      "metadata": {
        "id": "t68UArj6XDsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "URL_AIRPORTS = \"https://davidmegginson.github.io/ourairports-data/airports.csv\"\n",
        "\n",
        "# 1) Descargar desde la URL (queda en memoria como DataFrame)\n",
        "df_airports = pd.read_csv(URL_AIRPORTS)  # pandas permite leer CSV directo desde URL [web:497]\n",
        "\n",
        "df_airports.head()"
      ],
      "metadata": {
        "id": "sSyAxdH_moM2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:07.574991Z",
          "iopub.execute_input": "2026-01-10T14:11:07.575359Z",
          "iopub.status.idle": "2026-01-10T14:11:09.104029Z",
          "shell.execute_reply.started": "2026-01-10T14:11:07.575328Z",
          "shell.execute_reply": "2026-01-10T14:11:09.103262Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports.info()"
      ],
      "metadata": {
        "id": "NEzRl-etxA0N",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:13.987416Z",
          "iopub.execute_input": "2026-01-10T14:11:13.988177Z",
          "iopub.status.idle": "2026-01-10T14:11:14.057939Z",
          "shell.execute_reply.started": "2026-01-10T14:11:13.988125Z",
          "shell.execute_reply": "2026-01-10T14:11:14.057079Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. üßπ Limpieza y preparaci√≥n de datos**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "30OTtzYyy8Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dejar solo los aeropuertos de aviacion civil con vuelos programados"
      ],
      "metadata": {
        "id": "7FSQIbIUXPfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) (Opcional) Filtrar para aviaci√≥n civil \"normal\" (excluye heliports, seaplane)\n",
        "allowed_types = {\"small_airport\", \"medium_airport\", \"large_airport\"}\n",
        "\n",
        "df_airports = df_airports[\n",
        "    df_airports[\"type\"].isin(allowed_types) &\n",
        "    df_airports[\"scheduled_service\"].eq(\"yes\")\n",
        "].copy()\n",
        "\n",
        "df_airports.head()"
      ],
      "metadata": {
        "id": "XSxtnSkOnAzd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:17.663203Z",
          "iopub.execute_input": "2026-01-10T14:11:17.663571Z",
          "iopub.status.idle": "2026-01-10T14:11:17.697045Z",
          "shell.execute_reply.started": "2026-01-10T14:11:17.663538Z",
          "shell.execute_reply": "2026-01-10T14:11:17.696179Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buscar la informaci√≥n de unos aeropuertos por nombres"
      ],
      "metadata": {
        "id": "z-2TXk1PXW35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r\"Odessa|Sheremet|Murmansk|Domodedovo International Airport|Murmansk Airport\"\n",
        "\n",
        "airport = df_airports.loc[\n",
        "    df_airports[\"name\"].astype(str).str.contains(pattern, case=False, na=False, regex=True)\n",
        "]\n",
        "\n",
        "airport"
      ],
      "metadata": {
        "id": "qFVV5t6uU3ea",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:21.518184Z",
          "iopub.execute_input": "2026-01-10T14:11:21.519232Z",
          "iopub.status.idle": "2026-01-10T14:11:21.547901Z",
          "shell.execute_reply.started": "2026-01-10T14:11:21.519193Z",
          "shell.execute_reply": "2026-01-10T14:11:21.546966Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar si la informaci√≥n de los aeropuertos tiene los valores faltantes\n",
        "\n",
        "- Si iata_code esta nulo, lo reemplazamos por un valor no nulo extraido de una de las siguentes columnas `icao_code`, `gps_code` o `local_code`"
      ],
      "metadata": {
        "id": "xsEU5QfN75Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports[\"airport_code\"] = (\n",
        "    df_airports[\"iata_code\"]\n",
        "      .fillna(df_airports[\"icao_code\"])\n",
        "      .fillna(df_airports[\"gps_code\"])\n",
        "      .fillna(df_airports[\"local_code\"])\n",
        ")\n",
        "\n",
        "df_airports[\"airport_code\"].isna().mean()"
      ],
      "metadata": {
        "id": "IkEXlu5WyXqT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:28.657964Z",
          "iopub.execute_input": "2026-01-10T14:11:28.658929Z",
          "iopub.status.idle": "2026-01-10T14:11:28.671220Z",
          "shell.execute_reply.started": "2026-01-10T14:11:28.658893Z",
          "shell.execute_reply": "2026-01-10T14:11:28.670069Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Muestra data con nulos"
      ],
      "metadata": {
        "id": "94DAbhd-wIao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = df_airports[\"airport_code\"].isna()\n",
        "\n",
        "# inspecci√≥n r√°pida\n",
        "df_airports.loc[mask, [\"ident\",\"type\",\"name\",\"iso_country\",\"scheduled_service\",\n",
        "                          \"iata_code\",\"icao_code\",\"gps_code\",\"local_code\"]].head(20)"
      ],
      "metadata": {
        "id": "HKCbHgtsy2VE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:32.080071Z",
          "iopub.execute_input": "2026-01-10T14:11:32.080460Z",
          "iopub.status.idle": "2026-01-10T14:11:32.099677Z",
          "shell.execute_reply.started": "2026-01-10T14:11:32.080428Z",
          "shell.execute_reply": "2026-01-10T14:11:32.098468Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extraer solo los aeropuertos con valores no nulos en el `airport_code`"
      ],
      "metadata": {
        "id": "6ppg-KRTXp0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports = df_airports.loc[~mask].copy()"
      ],
      "metadata": {
        "id": "8hC-nwnEy8tY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:39.198528Z",
          "iopub.execute_input": "2026-01-10T14:11:39.198870Z",
          "iopub.status.idle": "2026-01-10T14:11:39.208778Z",
          "shell.execute_reply.started": "2026-01-10T14:11:39.198841Z",
          "shell.execute_reply": "2026-01-10T14:11:39.207869Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports.info()"
      ],
      "metadata": {
        "id": "V4GNuMEvzCQn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:41.972489Z",
          "iopub.execute_input": "2026-01-10T14:11:41.972855Z",
          "iopub.status.idle": "2026-01-10T14:11:41.988870Z",
          "shell.execute_reply.started": "2026-01-10T14:11:41.972826Z",
          "shell.execute_reply": "2026-01-10T14:11:41.988041Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports[\"airport_code\"].isna().sum()"
      ],
      "metadata": {
        "id": "0qRm9shPzUGT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:45.665989Z",
          "iopub.execute_input": "2026-01-10T14:11:45.666729Z",
          "iopub.status.idle": "2026-01-10T14:11:45.674225Z",
          "shell.execute_reply.started": "2026-01-10T14:11:45.666692Z",
          "shell.execute_reply": "2026-01-10T14:11:45.672918Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports[\"airport_code\"].sample(10)"
      ],
      "metadata": {
        "id": "kaCG3sWhzWmp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T13:57:36.109065Z",
          "iopub.execute_input": "2026-01-10T13:57:36.109416Z",
          "iopub.status.idle": "2026-01-10T13:57:36.118121Z",
          "shell.execute_reply.started": "2026-01-10T13:57:36.109387Z",
          "shell.execute_reply": "2026-01-10T13:57:36.117165Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtrar los aeropuertos de los EEUU"
      ],
      "metadata": {
        "id": "aMZEBiu6wp_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# airports: DataFrame ya cargado desde airports.csv (OurAirports)\n",
        "# airports = pd.read_csv(URL_AIRPORTS)\n",
        "\n",
        "# 1) Filtrar solo filas donde iso_region empieza con \"US-\"\n",
        "df_us = df_airports.loc[\n",
        "    df_airports[\"iso_region\"].astype(str).str.startswith(\"US-\"),\n",
        "].copy()  # str.startswith para filtrar prefijos [web:526]\n",
        "\n",
        "# 2) Crear state_clean = parte despu√©s del guion (US-AL -> AL)\n",
        "df_us[\"state_clean\"] = (\n",
        "    df_us[\"iso_region\"]\n",
        "      .astype(str)\n",
        "      .str.split(\"-\", n=1, expand=True)[1]\n",
        "      .str.strip()\n",
        ")  # split con expand para crear columnas [web:412][web:405]\n",
        "\n",
        "# 3) city_clean = municipality (solo renombre/copia)\n",
        "df_us[\"city_clean\"] = df_us[\"municipality\"].astype(str)\n",
        "df_us[\"latitude\"] = df_us[\"latitude_deg\"]\n",
        "df_us[\"longitude\"] = df_us[\"longitude_deg\"]\n",
        "\n",
        "# 4) Seleccionar columnas finales\n",
        "df_export = df_us[[\n",
        "    \"type\", \"name\", \"latitude\", \"longitude\",\n",
        "    \"state_clean\", \"city_clean\", \"airport_code\"\n",
        "]].copy()\n"
      ],
      "metadata": {
        "id": "BBZLcgqrpe7L",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:11:57.067504Z",
          "iopub.execute_input": "2026-01-10T14:11:57.067809Z",
          "iopub.status.idle": "2026-01-10T14:11:57.092086Z",
          "shell.execute_reply.started": "2026-01-10T14:11:57.067784Z",
          "shell.execute_reply": "2026-01-10T14:11:57.091179Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar la muestra de c√≥odigos de aeropuertos"
      ],
      "metadata": {
        "id": "WJ9S7bDkw3Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_export.head()"
      ],
      "metadata": {
        "id": "u7m2Avfdw2cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardar aeropuertos limpiados en un archivo CSV"
      ],
      "metadata": {
        "id": "LWu_oS0_X3dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Exportar a CSV\n",
        "df_export.to_csv(\"airports_us_clean.csv\", index=False)"
      ],
      "metadata": {
        "id": "1Azv6ExjsZge",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:12:01.128612Z",
          "iopub.execute_input": "2026-01-10T14:12:01.128959Z",
          "iopub.status.idle": "2026-01-10T14:12:01.144347Z",
          "shell.execute_reply.started": "2026-01-10T14:12:01.128929Z",
          "shell.execute_reply": "2026-01-10T14:12:01.143354Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. üõ¨ Cargar los aeropuertos depurados en un DataFrame y validarlos**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N2fAS9b9xYsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leer el archivo de aeropuertos"
      ],
      "metadata": {
        "id": "z74_BtlqyZOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_airports = pd.read_csv(\"airports_us_clean.csv\")\n",
        "df_airports.head()"
      ],
      "metadata": {
        "id": "eA_hf6Eqiw9M",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:12:13.071877Z",
          "iopub.execute_input": "2026-01-10T14:12:13.072269Z",
          "iopub.status.idle": "2026-01-10T14:12:13.089801Z",
          "shell.execute_reply.started": "2026-01-10T14:12:13.072237Z",
          "shell.execute_reply": "2026-01-10T14:12:13.088989Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unir el dataset de vuelos con las ubicaciones de los aeropuertos"
      ],
      "metadata": {
        "id": "8OZ5eF1vX8Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merge coordenadas y dataset\n",
        "\n",
        "df = df.merge(df_airports, on=[\"city_clean\", \"state_clean\"], how=\"left\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cTKHswaSouvK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:12:16.756027Z",
          "iopub.execute_input": "2026-01-10T14:12:16.756407Z",
          "iopub.status.idle": "2026-01-10T14:12:35.797479Z",
          "shell.execute_reply.started": "2026-01-10T14:12:16.756379Z",
          "shell.execute_reply": "2026-01-10T14:12:35.796440Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "jEBiQUotQvKs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:12:40.662035Z",
          "iopub.execute_input": "2026-01-10T14:12:40.662397Z",
          "iopub.status.idle": "2026-01-10T14:12:40.668610Z",
          "shell.execute_reply.started": "2026-01-10T14:12:40.662367Z",
          "shell.execute_reply": "2026-01-10T14:12:40.667772Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "D3-jRKsrwxNi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:12:43.296911Z",
          "iopub.execute_input": "2026-01-10T14:12:43.297890Z",
          "iopub.status.idle": "2026-01-10T14:12:43.341410Z",
          "shell.execute_reply.started": "2026-01-10T14:12:43.297773Z",
          "shell.execute_reply": "2026-01-10T14:12:43.337468Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminar valores nulos en los siguientes campos: `type`, `name`, `latitude`, `longitude`"
      ],
      "metadata": {
        "id": "tdAPYrn_ymL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_req = [\"type\", \"name\", \"latitude\", \"longitude\"]\n",
        "\n",
        "print(\"Antes:\", df.shape)\n",
        "df_base = df.dropna(subset=cols_req, how=\"any\").reset_index(drop=True)\n",
        "print(\"Despu√©s:\", df_base.shape)"
      ],
      "metadata": {
        "id": "GX0F9fodcA5C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:12:50.061946Z",
          "iopub.execute_input": "2026-01-10T14:12:50.063333Z",
          "iopub.status.idle": "2026-01-10T14:13:17.625748Z",
          "shell.execute_reply.started": "2026-01-10T14:12:50.063283Z",
          "shell.execute_reply": "2026-01-10T14:13:17.624687Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tomamar una fracci√≥n del dataset por cada a√±o, porque consultar la API del clima es costoso en tiempo de ejecuci√≥n"
      ],
      "metadata": {
        "id": "S5U7L7SP9mfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsample MVP\n",
        "N = 20_000\n",
        "frac = N / len(df)\n",
        "\n",
        "df_base = (\n",
        "    df.groupby(\"year\", group_keys=False)\n",
        "      .sample(frac=frac, random_state=42)\n",
        ")\n",
        "\n",
        "df_base.head()\n"
      ],
      "metadata": {
        "id": "TpH4X2EiQlZn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:13:17.627564Z",
          "iopub.execute_input": "2026-01-10T14:13:17.627845Z",
          "iopub.status.idle": "2026-01-10T14:13:26.706997Z",
          "shell.execute_reply.started": "2026-01-10T14:13:17.627819Z",
          "shell.execute_reply": "2026-01-10T14:13:26.706139Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardamos esa fracci√≥n en un archivo CSV"
      ],
      "metadata": {
        "id": "dIeH6vMXzony"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Exportar a CSV\n",
        "df_base.to_csv(\"df_base_merged_sample_and_airports.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:14:39.822840Z",
          "iopub.execute_input": "2026-01-10T14:14:39.823237Z",
          "iopub.status.idle": "2026-01-10T14:14:40.375335Z",
          "shell.execute_reply.started": "2026-01-10T14:14:39.823208Z",
          "shell.execute_reply": "2026-01-10T14:14:40.374532Z"
        },
        "id": "QAXjxv7ljl5H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. üå¶Ô∏è Consultar la API externa de clima hist√≥rico y generar un dataset de entrenamiento enriquecido**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FBXT-z7Lrchk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enriquecer el dataset de entrenamiento con informaci√≥n clim√°tica"
      ],
      "metadata": {
        "id": "ROQXYYWY0pH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos los resultados en bloques de 500 filas dentro del archivo `openmeteo_daily_cache.parquet`.\n",
        "\n",
        "Como este archivo queda en el almacenamiento temporal del notebook, es recomendable descargarlo peri√≥dicamente para evitar p√©rdidas si la ejecuci√≥n se interrumpe y as√≠ poder retomar el proceso sin repetir consultas a la API del clima.\n",
        "\n",
        "Para continuar, vuelve a subir `openmeteo_daily_cache.parquet` al entorno temporal del notebook y ejecuta nuevamente las celdas siguientes, de modo que el proceso siga escribiendo sobre la cach√© existente.\n",
        "\n",
        "La constante `HARD_CAP = 5_000` indica el n√∫mero m√°ximo total de consultas permitidas a la API del clima; as√≠ evitas ejecuciones muy largas o consumir demasiada cuota.\n",
        "\n",
        "Asigna el valor seg√∫n tus necesidades (por ejemplo, `HARD_CAP = 5_000` o `HARD_CAP = 15_000`) y ajusta en base al tiempo/costo que est√°s dispuesto a asumir; una buena pr√°ctica es partir conservador y luego aumentar si hace falta."
      ],
      "metadata": {
        "id": "SFlVP0rX632Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# c√≥digo completo incorporando selecci√≥n balanceada por a√±o para keys_missing,\n",
        "# de modo que cuando pidas max_api_calls=15000 no se vaya todo al primer a√±o (p. ej. 2018),\n",
        "# sino que tome una cantidad similar por a√±o (y si un a√±o tiene pocas claves faltantes, toma todas y redistribuye el resto).\n",
        "#  La parte clave es usar groupby(...).apply(lambda g: g.sample(...)) para muestrear por grupo/a√±o.\n",
        "\n",
        "BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "HARD_CAP = 20_000          # l√≠mite duro (tu tope global) ‚Äî respeta tu configuraci√≥n\n",
        "DEFAULT_TIMEOUT = 15\n",
        "MAX_RETRIES = 3\n",
        "BACKOFF_BASE = 0.75        # segundos\n",
        "WRITE_BATCH_SIZE = 500     # cada cu√°ntas respuestas persistimos\n",
        "\n",
        "\n",
        "def _safe_sleep(last_call_ts, min_interval_s):\n",
        "    now = time.time()\n",
        "    wait = (last_call_ts + min_interval_s) - now\n",
        "    if wait > 0:\n",
        "        time.sleep(wait)\n",
        "    return time.time()\n",
        "\n",
        "\n",
        "def _retry_get(session, url, params, timeout=DEFAULT_TIMEOUT, max_retries=MAX_RETRIES):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            r = session.get(url, params=params, timeout=timeout)\n",
        "            r.raise_for_status()\n",
        "            return r\n",
        "        except requests.RequestException:\n",
        "            if attempt == max_retries:\n",
        "                raise\n",
        "            back = BACKOFF_BASE * (2 ** (attempt - 1))\n",
        "            time.sleep(back)\n",
        "    raise RuntimeError(\"Exhausted retries unexpectedly\")\n",
        "\n",
        "\n",
        "def fetch_daily_weather_for_point_date(lat, lon, date_str, timezone=\"UTC\", session=None, timeout=DEFAULT_TIMEOUT):\n",
        "    params = {\n",
        "        \"latitude\": float(lat),\n",
        "        \"longitude\": float(lon),\n",
        "        \"start_date\": date_str,\n",
        "        \"end_date\": date_str,\n",
        "        \"daily\": \"temperature_2m_max,temperature_2m_min,precipitation_sum,windspeed_10m_max\",\n",
        "        \"timezone\": timezone,\n",
        "    }\n",
        "    s = session or requests.Session()\n",
        "    r = _retry_get(s, BASE_URL, params=params, timeout=timeout, max_retries=MAX_RETRIES)\n",
        "    data = r.json()\n",
        "\n",
        "    d = data.get(\"daily\", {})\n",
        "    row = {\n",
        "        \"flightdate\": date_str,\n",
        "        \"temp_max\": d.get(\"temperature_2m_max\", [None])[0],\n",
        "        \"temp_min\": d.get(\"temperature_2m_min\", [None])[0],\n",
        "        \"precipitation_sum\": d.get(\"precipitation_sum\", [None])[0],\n",
        "        \"wind_speed_max\": d.get(\"windspeed_10m_max\", [None])[0],\n",
        "    }\n",
        "    if row[\"temp_max\"] is not None and row[\"temp_min\"] is not None:\n",
        "        row[\"temp_mean\"] = (float(row[\"temp_max\"]) + float(row[\"temp_min\"])) / 2.0\n",
        "    else:\n",
        "        row[\"temp_mean\"] = None\n",
        "    return row\n",
        "\n",
        "\n",
        "def _ensure_dir(path):\n",
        "    d = os.path.dirname(os.path.abspath(path))\n",
        "    if d and not os.path.exists(d):\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "def _lock_file(path, mode=\"a+b\"):\n",
        "    f = open(path, mode)\n",
        "    try:\n",
        "        fcntl.flock(f.fileno(), fcntl.LOCK_EX)\n",
        "    except Exception:\n",
        "        f.close()\n",
        "        raise\n",
        "    return f\n",
        "\n",
        "\n",
        "def _write_parquet_atomic(df: pd.DataFrame, path: str, tmp_suffix=\".tmp\"):\n",
        "    tmp_path = f\"{path}{tmp_suffix}\"\n",
        "    df.to_parquet(tmp_path, index=False)\n",
        "    os.replace(tmp_path, path)\n",
        "\n",
        "\n",
        "def _load_cache(cache_path: str) -> pd.DataFrame:\n",
        "    cols = [\"_lat_r\",\"_lon_r\",\"flightdate\",\"temp_max\",\"temp_min\",\"temp_mean\",\"precipitation_sum\",\"wind_speed_max\"]\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            df_cache = pd.read_parquet(cache_path)\n",
        "            for c in cols:\n",
        "                if c not in df_cache.columns:\n",
        "                    df_cache[c] = pd.Series(dtype=\"float64\" if c.startswith(\"temp\") or c in [\"precipitation_sum\",\"wind_speed_max\"] else \"object\")\n",
        "            df_cache = df_cache.drop_duplicates(subset=[\"_lat_r\", \"_lon_r\", \"flightdate\"], keep=\"last\")\n",
        "            return df_cache\n",
        "        except Exception:\n",
        "            corrupt = cache_path + \".corrupt\"\n",
        "            os.replace(cache_path, corrupt)\n",
        "            print(f\"[cache] Archivo corrupto movido a: {corrupt}. Se reinicia cache.\")\n",
        "    return pd.DataFrame(columns=cols)\n",
        "\n",
        "\n",
        "def _balance_keys_by_year(keys_missing: pd.DataFrame, max_api_calls: int, random_state: int = 42, exclude_years=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve un subconjunto de keys_missing, muestreado de forma balanceada por 'year'\n",
        "    derivado desde flightdate. Usa groupby+sample. [web:696][web:712]\n",
        "    \"\"\"\n",
        "    exclude_years = set(exclude_years or [])\n",
        "    km = keys_missing.copy()\n",
        "    km[\"year\"] = pd.to_datetime(km[\"flightdate\"], errors=\"coerce\").dt.year\n",
        "\n",
        "    if exclude_years:\n",
        "        km = km[~km[\"year\"].isin(exclude_years)].copy()\n",
        "\n",
        "    # Si no hay year parseable, fallback a sample global\n",
        "    years = sorted(km[\"year\"].dropna().unique())\n",
        "    if len(years) == 0:\n",
        "        out = km.sample(n=min(len(km), max_api_calls), random_state=random_state)\n",
        "        return out.drop(columns=[\"year\"], errors=\"ignore\")\n",
        "\n",
        "    # Target por a√±o\n",
        "    per_year = math.ceil(max_api_calls / len(years))\n",
        "\n",
        "    # Sample por grupo/a√±o (sin reemplazo; si hay menos, toma todo)\n",
        "    sampled = (\n",
        "        km.sample(frac=1, random_state=random_state)\n",
        "          .groupby(\"year\", group_keys=False)\n",
        "          .head(per_year)\n",
        "    )\n",
        "\n",
        "    # Si sobraron por ceil, recorta a max_api_calls; si faltaron, rellena con el resto\n",
        "    if len(sampled) > max_api_calls:\n",
        "        sampled = sampled.sample(n=max_api_calls, random_state=random_state)\n",
        "\n",
        "    elif len(sampled) < max_api_calls:\n",
        "        picked_keys = (\n",
        "            sampled[\"_lat_r\"].astype(str) + \"|\" +\n",
        "            sampled[\"_lon_r\"].astype(str) + \"|\" +\n",
        "            sampled[\"flightdate\"].astype(str)\n",
        "        )\n",
        "        picked_set = set(picked_keys.values)\n",
        "\n",
        "        all_keys = (\n",
        "            km[\"_lat_r\"].astype(str) + \"|\" +\n",
        "            km[\"_lon_r\"].astype(str) + \"|\" +\n",
        "            km[\"flightdate\"].astype(str)\n",
        "        )\n",
        "        rest = km.loc[~all_keys.isin(picked_set)]\n",
        "        need = max_api_calls - len(sampled)\n",
        "        if len(rest) > 0 and need > 0:\n",
        "            sampled = pd.concat(\n",
        "                [sampled, rest.sample(n=min(len(rest), need), random_state=random_state)],\n",
        "                ignore_index=True\n",
        "            )\n",
        "\n",
        "    return sampled.drop(columns=[\"year\"], errors=\"ignore\")\n",
        "\n",
        "\n",
        "def enrich_df_base_with_weather_daily_cached(\n",
        "    df_base: pd.DataFrame,\n",
        "    timezone=\"UTC\",\n",
        "    max_requests_per_second=2.0,\n",
        "    max_api_calls=20000,\n",
        "    round_coords_decimals=3,\n",
        "    cache_path=\"/content/drive/MyDrive/openmeteo_daily_cache.parquet\",\n",
        "    show_progress=True,\n",
        "    random_state=42,\n",
        "    exclude_years=None,        # ej: [2018] si quieres saltarte 2018\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    max_api_calls = min(int(max_api_calls), HARD_CAP)\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    # Normaliza fechas y claves\n",
        "    df = df_base.copy()\n",
        "    df[\"flightdate\"] = pd.to_datetime(df[\"flightdate\"]).dt.strftime(\"%Y-%m-%d\")\n",
        "    df[\"_lat_r\"] = df[\"latitude\"].round(round_coords_decimals)\n",
        "    df[\"_lon_r\"] = df[\"longitude\"].round(round_coords_decimals)\n",
        "\n",
        "    keys = (\n",
        "        df.loc[df[\"_lat_r\"].notna() & df[\"_lon_r\"].notna(), [\"_lat_r\", \"_lon_r\", \"flightdate\"]]\n",
        "          .drop_duplicates()\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Carga cache\n",
        "    df_cache = _load_cache(cache_path)\n",
        "\n",
        "    # Anti-join vectorizado\n",
        "    if not df_cache.empty:\n",
        "        cache_key = (\n",
        "            df_cache[\"_lat_r\"].astype(str) + \"|\" +\n",
        "            df_cache[\"_lon_r\"].astype(str) + \"|\" +\n",
        "            df_cache[\"flightdate\"].astype(str)\n",
        "        )\n",
        "        cached_set = set(cache_key.values)\n",
        "    else:\n",
        "        cached_set = set()\n",
        "\n",
        "    keys_key = keys[\"_lat_r\"].astype(str) + \"|\" + keys[\"_lon_r\"].astype(str) + \"|\" + keys[\"flightdate\"].astype(str)\n",
        "    missing_mask = ~keys_key.isin(cached_set)\n",
        "    keys_missing = keys.loc[missing_mask].reset_index(drop=True)\n",
        "\n",
        "    total_missing = len(keys_missing)\n",
        "    if total_missing == 0:\n",
        "        print(f\"[ok] No hay claves faltantes. Cache: {len(df_cache)} filas. Archivo: {os.path.abspath(cache_path)}\")\n",
        "\n",
        "    # IMPORTANTE: balancea por a√±o ANTES de aplicar el cap final\n",
        "    if total_missing > 0:\n",
        "        keys_missing = _balance_keys_by_year(\n",
        "            keys_missing=keys_missing,\n",
        "            max_api_calls=max_api_calls,\n",
        "            random_state=random_state,\n",
        "            exclude_years=exclude_years\n",
        "        )\n",
        "\n",
        "    to_fetch = len(keys_missing)\n",
        "    print(f\"[plan] √önicas totales: {len(keys)} | En cache: {len(cached_set)} | A consultar: {to_fetch} (cap m√°x: {HARD_CAP})\")\n",
        "\n",
        "    if to_fetch == 0:\n",
        "        df_enriched = df.merge(\n",
        "            df_cache.drop_duplicates(subset=[\"_lat_r\",\"_lon_r\",\"flightdate\"], keep=\"last\"),\n",
        "            how=\"left\",\n",
        "            on=[\"_lat_r\",\"_lon_r\",\"flightdate\"]\n",
        "        )\n",
        "        df_enriched.drop(columns=[\"_lat_r\", \"_lon_r\"], inplace=True)\n",
        "        return df_enriched\n",
        "\n",
        "    # Rate limit\n",
        "    min_interval_s = 1.0 / max_requests_per_second if max_requests_per_second > 0 else 0.0\n",
        "    last_call_ts = 0.0\n",
        "    session = requests.Session()\n",
        "\n",
        "    # Barra de progreso opcional\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        bar = tqdm(total=to_fetch, unit=\"req\", disable=not show_progress)\n",
        "    except Exception:\n",
        "        bar = None\n",
        "        print(\"[info] tqdm no disponible. Continuando sin barra de progreso.\")\n",
        "\n",
        "    new_rows = []\n",
        "    written_since_last = 0\n",
        "    processed = 0\n",
        "    start_ts = time.time()\n",
        "\n",
        "    def persist_incremental(df_cache_local, new_rows_batch):\n",
        "        if not new_rows_batch:\n",
        "            return df_cache_local\n",
        "        #df_new_local = pd.DataFrame(new_rows_batch)\n",
        "        #df_cache_local = pd.concat([df_cache_local, df_new_local], ignore_index=True)\n",
        "        df_new_local = pd.DataFrame(new_rows_batch)\n",
        "        if df_new_local.empty:\n",
        "            return df_cache_local\n",
        "\n",
        "        df_cache_local = pd.concat([df_cache_local, df_new_local], ignore_index=True)\n",
        "        df_cache_local = df_cache_local.drop_duplicates(subset=[\"_lat_r\",\"_lon_r\",\"flightdate\"], keep=\"last\")\n",
        "        with _lock_file(cache_path, mode=\"a+b\"):\n",
        "            _write_parquet_atomic(df_cache_local, cache_path)\n",
        "        return df_cache_local\n",
        "\n",
        "    for _, k in keys_missing.iterrows():\n",
        "        if processed >= HARD_CAP:\n",
        "            break\n",
        "\n",
        "        last_call_ts = _safe_sleep(last_call_ts, min_interval_s)\n",
        "\n",
        "        try:\n",
        "            row = fetch_daily_weather_for_point_date(\n",
        "                lat=k[\"_lat_r\"], lon=k[\"_lon_r\"], date_str=k[\"flightdate\"],\n",
        "                timezone=timezone, session=session\n",
        "            )\n",
        "            row[\"_lat_r\"] = k[\"_lat_r\"]\n",
        "            row[\"_lon_r\"] = k[\"_lon_r\"]\n",
        "        except requests.RequestException:\n",
        "            row = {\n",
        "                \"_lat_r\": k[\"_lat_r\"],\n",
        "                \"_lon_r\": k[\"_lon_r\"],\n",
        "                \"flightdate\": k[\"flightdate\"],\n",
        "                \"temp_max\": None,\n",
        "                \"temp_min\": None,\n",
        "                \"temp_mean\": None,\n",
        "                \"precipitation_sum\": None,\n",
        "                \"wind_speed_max\": None,\n",
        "            }\n",
        "\n",
        "        new_rows.append(row)\n",
        "        processed += 1\n",
        "        written_since_last += 1\n",
        "\n",
        "        if bar:\n",
        "            bar.update(1)\n",
        "            elapsed = max(time.time() - start_ts, 1e-6)\n",
        "            rate = processed / elapsed\n",
        "            remaining = to_fetch - processed\n",
        "            eta = remaining / rate if rate > 0 else float(\"inf\")\n",
        "            bar.set_postfix({\"rate\": f\"{rate:.2f}/s\", \"ETA\": f\"{eta/60:.1f}m\"})\n",
        "\n",
        "        if written_since_last >= WRITE_BATCH_SIZE:\n",
        "            df_cache = persist_incremental(df_cache, new_rows)\n",
        "            print(f\"[persist] Escrito batch de {written_since_last}. Total cache: {len(df_cache)}\")\n",
        "            new_rows.clear()\n",
        "            written_since_last = 0\n",
        "\n",
        "    if new_rows:\n",
        "        df_cache = persist_incremental(df_cache, new_rows)\n",
        "        print(f\"[persist] Escrito batch final de {len(new_rows)}. Total cache: {len(df_cache)}\")\n",
        "        new_rows.clear()\n",
        "\n",
        "    if bar:\n",
        "        bar.close()\n",
        "\n",
        "    # Merge final\n",
        "    df_enriched = df.merge(\n",
        "        df_cache.drop_duplicates(subset=[\"_lat_r\",\"_lon_r\",\"flightdate\"], keep=\"last\"),\n",
        "        how=\"left\",\n",
        "        on=[\"_lat_r\",\"_lon_r\",\"flightdate\"]\n",
        "    )\n",
        "    df_enriched.drop(columns=[\"_lat_r\", \"_lon_r\"], inplace=True)\n",
        "\n",
        "    print(f\"[done] Enriquecidas {len(df_enriched)} filas. Cache en: {os.path.abspath(cache_path)}\")\n",
        "    return df_enriched"
      ],
      "metadata": {
        "id": "g0eX0mBk62tv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:15:14.562249Z",
          "iopub.execute_input": "2026-01-10T14:15:14.562688Z",
          "iopub.status.idle": "2026-01-10T14:15:14.602725Z",
          "shell.execute_reply.started": "2026-01-10T14:15:14.562650Z",
          "shell.execute_reply": "2026-01-10T14:15:14.601749Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejecutar la funci√≥n que consulta la API y actualiza el archivo de cach√©"
      ],
      "metadata": {
        "id": "ESRQd_K6apgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_base = enrich_df_base_with_weather_daily_cached(\n",
        "    df_base,\n",
        "    timezone=\"auto\",\n",
        "    max_requests_per_second=2.0,\n",
        "    max_api_calls=15000,\n",
        "    round_coords_decimals=3,\n",
        "    cache_path=\"openmeteo_daily_cache.parquet\",\n",
        "    #exclude_years=[2018, 2019], # opcional. Si ya tienes info de 2018 y 2019 y quieres priorizar otros a√±os\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a3w22bkNyZyp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T14:15:33.608333Z",
          "iopub.execute_input": "2026-01-10T14:15:33.609514Z",
          "iopub.status.idle": "2026-01-10T19:27:16.512896Z",
          "shell.execute_reply.started": "2026-01-10T14:15:33.609471Z",
          "shell.execute_reply": "2026-01-10T19:27:16.512001Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exportar el `DataFrame` a CSV: `dataset_con_meteo_out.csv`"
      ],
      "metadata": {
        "id": "cmxzhM8dbd0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_base.to_csv(\"dataset_con_meteo_out.csv\", index=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "tMJ3m3E46f9t",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar el CSV en un nuevo `DataFrame`"
      ],
      "metadata": {
        "id": "AcpUx1-vbhNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset_con_meteo_out.csv\")"
      ],
      "metadata": {
        "id": "vGWJBChO12FK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:28:13.067956Z",
          "iopub.execute_input": "2026-01-10T19:28:13.068800Z",
          "iopub.status.idle": "2026-01-10T19:28:13.354644Z",
          "shell.execute_reply.started": "2026-01-10T19:28:13.068762Z",
          "shell.execute_reply": "2026-01-10T19:28:13.353762Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspeccionar `DataFrame`\n"
      ],
      "metadata": {
        "id": "wFXLhoE-4CBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HPUdt9gc2ADf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:28:16.905302Z",
          "iopub.execute_input": "2026-01-10T19:28:16.906388Z",
          "iopub.status.idle": "2026-01-10T19:28:16.941755Z",
          "shell.execute_reply.started": "2026-01-10T19:28:16.906328Z",
          "shell.execute_reply": "2026-01-10T19:28:16.940363Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AFaVW4Hy2LqU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:28:22.156772Z",
          "iopub.execute_input": "2026-01-10T19:28:22.157467Z",
          "iopub.status.idle": "2026-01-10T19:28:22.179507Z",
          "shell.execute_reply.started": "2026-01-10T19:28:22.157435Z",
          "shell.execute_reply": "2026-01-10T19:28:22.178398Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Ru6e_E5X3-Nn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:29:17.111191Z",
          "iopub.execute_input": "2026-01-10T19:29:17.112378Z",
          "iopub.status.idle": "2026-01-10T19:29:17.118088Z",
          "shell.execute_reply.started": "2026-01-10T19:29:17.112339Z",
          "shell.execute_reply": "2026-01-10T19:29:17.116889Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Renombrar las nuevas columnas agregadas previamente"
      ],
      "metadata": {
        "id": "m0JjsP0tbmvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={\n",
        "    \"precipitation_sum\": \"precipitation\",\n",
        "    \"wind_speed_max\": \"wind_speed\",\n",
        "})"
      ],
      "metadata": {
        "id": "iPKRYAk22VsO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:29:21.570823Z",
          "iopub.execute_input": "2026-01-10T19:29:21.571176Z",
          "iopub.status.idle": "2026-01-10T19:29:21.578767Z",
          "shell.execute_reply.started": "2026-01-10T19:29:21.571146Z",
          "shell.execute_reply": "2026-01-10T19:29:21.577948Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar cu√°ntos valores nulos tienen estas columnas"
      ],
      "metadata": {
        "id": "9diMJtkGbuFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"temp_mean\", \"precipitation\", \"wind_speed\"]].isna().sum()"
      ],
      "metadata": {
        "id": "z7KXSuiu3X7U",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:29:36.173653Z",
          "iopub.execute_input": "2026-01-10T19:29:36.174010Z",
          "iopub.status.idle": "2026-01-10T19:29:36.183958Z",
          "shell.execute_reply.started": "2026-01-10T19:29:36.173982Z",
          "shell.execute_reply": "2026-01-10T19:29:36.182778Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar cu√°ntos valores nulos tiene el dataset, agrupado por a√±o"
      ],
      "metadata": {
        "id": "DndBLp0Tbzc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# null por a√±os\n",
        "cols = [\"temp_mean\", \"precipitation\", \"wind_speed\"]\n",
        "\n",
        "mask_any_na = df[cols].isna().any(axis=1)\n",
        "na_rows_by_year = df.loc[mask_any_na].groupby(\"year\").size().sort_index()\n",
        "\n",
        "na_rows_by_year"
      ],
      "metadata": {
        "id": "LLS2dUOT45m6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:29:44.563514Z",
          "iopub.execute_input": "2026-01-10T19:29:44.563885Z",
          "iopub.status.idle": "2026-01-10T19:29:44.578039Z",
          "shell.execute_reply.started": "2026-01-10T19:29:44.563854Z",
          "shell.execute_reply": "2026-01-10T19:29:44.577084Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar cu√°ntos valores no nulos tenemos por a√±o"
      ],
      "metadata": {
        "id": "4laUTwDTfb99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No-null por a√±o (para columnas espec√≠ficas)\n",
        "\n",
        "cols = [\"temp_mean\", \"precipitation\", \"wind_speed\"]\n",
        "\n",
        "not_null_by_year = df.groupby(\"year\")[cols].count().sort_index()\n",
        "not_null_by_year"
      ],
      "metadata": {
        "id": "UgqRXf0A5TeE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:30:10.210527Z",
          "iopub.execute_input": "2026-01-10T19:30:10.211327Z",
          "iopub.status.idle": "2026-01-10T19:30:10.224261Z",
          "shell.execute_reply.started": "2026-01-10T19:30:10.211289Z",
          "shell.execute_reply": "2026-01-10T19:30:10.223509Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar una muestra de los datos con valores nulos"
      ],
      "metadata": {
        "id": "928aH2oWb8oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[mask_any_na, [\"year\", \"flightdate\"] + cols].sample(20)"
      ],
      "metadata": {
        "id": "okqN_uqz4uww",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:30:41.708841Z",
          "iopub.execute_input": "2026-01-10T19:30:41.709180Z",
          "iopub.status.idle": "2026-01-10T19:30:41.725321Z",
          "shell.execute_reply.started": "2026-01-10T19:30:41.709151Z",
          "shell.execute_reply": "2026-01-10T19:30:41.724519Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminar todos los registros con nulos en las columnas nuevas y conservar solo los que tienen informaci√≥n meteorol√≥gica; revisar el resultado"
      ],
      "metadata": {
        "id": "MqVD7GntcBgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos filas sin informaci√≥n clim√°tica\n",
        "\n",
        "df_ml = df.dropna(\n",
        "    subset=[\"temp_mean\", \"precipitation\", \"wind_speed\"]\n",
        ")\n",
        "\n",
        "df_ml.shape"
      ],
      "metadata": {
        "id": "G0YlDZ9W2Q7U",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:31:27.712813Z",
          "iopub.execute_input": "2026-01-10T19:31:27.713190Z",
          "iopub.status.idle": "2026-01-10T19:31:27.727589Z",
          "shell.execute_reply.started": "2026-01-10T19:31:27.713156Z",
          "shell.execute_reply": "2026-01-10T19:31:27.726303Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.info()"
      ],
      "metadata": {
        "id": "i2VmAv1nORAH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:31:38.325804Z",
          "iopub.execute_input": "2026-01-10T19:31:38.326148Z",
          "iopub.status.idle": "2026-01-10T19:31:38.347085Z",
          "shell.execute_reply.started": "2026-01-10T19:31:38.326120Z",
          "shell.execute_reply": "2026-01-10T19:31:38.346252Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminar las columnas sobrantes y revisar el resultado"
      ],
      "metadata": {
        "id": "sSDAWVb047mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml = df_ml.drop(columns=[\"temp_min\", \"temp_max\"])"
      ],
      "metadata": {
        "id": "hclg-6WpO-y2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:32:17.687581Z",
          "iopub.execute_input": "2026-01-10T19:32:17.687973Z",
          "iopub.status.idle": "2026-01-10T19:32:17.698606Z",
          "shell.execute_reply.started": "2026-01-10T19:32:17.687934Z",
          "shell.execute_reply": "2026-01-10T19:32:17.697705Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.info()"
      ],
      "metadata": {
        "id": "xCVOjI3yPc5v",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:36:22.684527Z",
          "iopub.execute_input": "2026-01-10T19:36:22.684888Z",
          "iopub.status.idle": "2026-01-10T19:36:22.706175Z",
          "shell.execute_reply.started": "2026-01-10T19:36:22.684857Z",
          "shell.execute_reply": "2026-01-10T19:36:22.705293Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisar y eliminar registros duplicados"
      ],
      "metadata": {
        "id": "5qUZ4VUE5fyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Buscando duplicados...\")\n",
        "num_duplicates = df_ml.duplicated().sum()\n",
        "print(f\"N√∫mero de filas duplicadas: {num_duplicates}\")"
      ],
      "metadata": {
        "id": "4LQEGYOSRfre",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:37:00.627451Z",
          "iopub.execute_input": "2026-01-10T19:37:00.627803Z",
          "iopub.status.idle": "2026-01-10T19:37:00.658307Z",
          "shell.execute_reply.started": "2026-01-10T19:37:00.627772Z",
          "shell.execute_reply": "2026-01-10T19:37:00.657352Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Registros antes de eliminar duplicados:\", len(df_ml))\n",
        "df_ml = df_ml.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
        "print(\"Registros despu√©s de eliminar duplicados:\", len(df_ml))"
      ],
      "metadata": {
        "id": "Aj56q4j_SOLy",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:36:56.378222Z",
          "iopub.execute_input": "2026-01-10T19:36:56.378526Z",
          "iopub.status.idle": "2026-01-10T19:36:56.415132Z",
          "shell.execute_reply.started": "2026-01-10T19:36:56.378502Z",
          "shell.execute_reply": "2026-01-10T19:36:56.414041Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asegurar que solo queden aeropuertos de aviaci√≥n civil con vuelos programados"
      ],
      "metadata": {
        "id": "K75eBsI16yrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allowed_types = {\"small_airport\", \"medium_airport\", \"large_airport\"}\n",
        "df_ml = df_ml[df_ml[\"type\"].isin(allowed_types)].copy()"
      ],
      "metadata": {
        "id": "krxmrebTT4YJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:37:21.331307Z",
          "iopub.execute_input": "2026-01-10T19:37:21.331721Z",
          "iopub.status.idle": "2026-01-10T19:37:21.341624Z",
          "shell.execute_reply.started": "2026-01-10T19:37:21.331690Z",
          "shell.execute_reply": "2026-01-10T19:37:21.340629Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.loc[df_ml[\"type\"].isin(allowed_types), \"type\"].unique()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:39:22.791474Z",
          "iopub.execute_input": "2026-01-10T19:39:22.791809Z",
          "iopub.status.idle": "2026-01-10T19:39:22.801962Z",
          "shell.execute_reply.started": "2026-01-10T19:39:22.791783Z",
          "shell.execute_reply": "2026-01-10T19:39:22.801192Z"
        },
        "id": "C_K6PvRSjl5f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.sample(20)\n"
      ],
      "metadata": {
        "id": "ErsO1VnbTHqm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:39:31.195963Z",
          "iopub.execute_input": "2026-01-10T19:39:31.196353Z",
          "iopub.status.idle": "2026-01-10T19:39:31.227851Z",
          "shell.execute_reply.started": "2026-01-10T19:39:31.196313Z",
          "shell.execute_reply": "2026-01-10T19:39:31.226893Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.info()"
      ],
      "metadata": {
        "id": "9TsSJri_Uhso",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:39:38.203028Z",
          "iopub.execute_input": "2026-01-10T19:39:38.203470Z",
          "iopub.status.idle": "2026-01-10T19:39:38.224474Z",
          "shell.execute_reply.started": "2026-01-10T19:39:38.203438Z",
          "shell.execute_reply": "2026-01-10T19:39:38.223574Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. ‚¨áÔ∏è Exportar el dataset final a CSV y forzar su descarga (solo en Colab)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SfYrQEDRcOZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CSV = \"dataset_vuelos_clima_final.csv\"\n",
        "df_ml.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "OUTPUT_CSV\n",
        "\n",
        "# Si tienes recursos suficientes y est√°s ejecutando este notebook en Google Colab,\n",
        "# puedes descomentar el bloque de abajo para forzar la descarga del archivo generado.\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_CSV)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dY6QmOmuVVVr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-10T19:39:50.456914Z",
          "iopub.execute_input": "2026-01-10T19:39:50.457289Z",
          "iopub.status.idle": "2026-01-10T19:39:50.839175Z",
          "shell.execute_reply.started": "2026-01-10T19:39:50.457260Z",
          "shell.execute_reply": "2026-01-10T19:39:50.838376Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}